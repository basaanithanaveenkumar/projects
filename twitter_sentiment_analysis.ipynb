{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_sentiment_analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj3erD7Cnf3Y"
      },
      "source": [
        "# install the required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzuYcGe4nfkJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQzCQp3nad5"
      },
      "source": [
        " data=pd.read_csv(\"train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGgKLDYpqywE"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K94hw4mXq3OJ"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni5MEQvT-Zxb"
      },
      "source": [
        "data['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yzCm5qB-ZuF"
      },
      "source": [
        "data['sentiment'].value_counts().plot(kind = 'barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfWW9lpMiw8W"
      },
      "source": [
        "# its an imbalanced data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0eamfmLjT1-"
      },
      "source": [
        "#percentage\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRJdKRfoivDF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzkX6gcV-ZsW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIThPBWA-ZoX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eYhcqbB-ZnB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4r1gRzp-Zjf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_FwVRJ7-Zh7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRdqFTGW-Ze8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWKpuRur-Zc7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqFbuDQQ-ZZv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN_SqTd--ZXn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaEbTGTZrOvr"
      },
      "source": [
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OStkbjoqrRh8"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k2oPm-mrToy"
      },
      "source": [
        "data=data[['text','selected_text','sentiment']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZHJeXAUr-Xg"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-k9wOelr_Xl"
      },
      "source": [
        "data.isnull()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr9s4S5LsdNk"
      },
      "source": [
        "null_columns=data.columns[data.isnull().any()]\n",
        "data[null_columns].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmqSU0dQsLUw"
      },
      "source": [
        "print(data[data.isnull().any(axis=1)][null_columns].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPAWEroPzWl6"
      },
      "source": [
        "labels = np.array(data['sentiment'])\n",
        "y = []\n",
        "for i in range(len(labels)):\n",
        "    if labels[i] == 'neutral':\n",
        "        y.append(0)\n",
        "    if labels[i] == 'negative':\n",
        "        y.append(1)\n",
        "    if labels[i] == 'positive':\n",
        "        y.append(2)\n",
        "y = np.array(y)\n",
        "labels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\n",
        "del y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-6fl-dgzhmx"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0MZw7IXtGWq"
      },
      "source": [
        "# code from Kaggle to clean data using regular expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmpe8yTdtGEB"
      },
      "source": [
        "import re\n",
        "def depure_data(data):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    data = url_pattern.sub(r'', data)\n",
        "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
        "    data = re.sub('\\s+', ' ', data)\n",
        "    data = re.sub(\"\\'\", \"\", data)   \n",
        "    return data\n",
        "\n",
        "t= []\n",
        "convert_to_list =data['selected_text'].values.tolist()\n",
        "for x in range(len(convert_to_list)):\n",
        "    t.append(depure_data(convert_to_list[x]))\n",
        "list(t[:5])\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) \n",
        "        \n",
        "\n",
        "data_words = list(sent_to_words(t))\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "print(data_words[:10])\n",
        "def detokenize(text):\n",
        "    return TreebankWordDetokenizer().detokenize(text)\n",
        "    \n",
        "dat = []\n",
        "for i in range(len(data_words)): \n",
        "    #for j in range(len(data_words[i])):\n",
        "        #stemming\n",
        "        #data_words[i][j] = lancaster.stem(data_words[i][j])\n",
        "        #lemmatization\n",
        "        #data_words[i][j] = data_words[i][j].format(data_words[i][j],wordnet_lemmatizer.lemmatize(data_words[i][j]))\n",
        "    \n",
        "    dat.append(detokenize(data_words[i]))\n",
        "print(dat[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9MDhEUFy4ee"
      },
      "source": [
        "\n",
        "dat = np.array(dat)\n",
        "dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMrmskm4fm1v"
      },
      "source": [
        "# Deep Learning LSTM models and GRU models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFWBLohAsr7d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "max_words = 5000\n",
        "max_len = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(dat)\n",
        "sequences = tokenizer.texts_to_sequences(dat)\n",
        "tweets = pad_sequences(sequences, maxlen=max_len)\n",
        "print(tweets)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSd09tY6w2KN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_zh7QP8yuHE"
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(layers.Embedding(max_words, 20))\n",
        "model1.add(layers.LSTM(15,dropout=0.5))\n",
        "model1.add(layers.Dense(3,activation='softmax'))\n",
        "\n",
        "\n",
        "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
        "history = model1.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test),callbacks=[checkpoint1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCxBR1TeYw2r"
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Jrhj9KWvY_"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai40j8aS0v7P"
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
        "model2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\n",
        "model2.add(layers.Dense(3,activation='softmax'))\n",
        "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "#model2 = keras.models.load_model('./best_model2.hdf5')\n",
        "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
        "history = model2.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test),callbacks=[checkpoint2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHnZy8LOnoI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjyGHdWgW1s8"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEqQJE0oVuZL"
      },
      "source": [
        "model5 = Sequential()\n",
        "model5.add(layers.Embedding(max_words, 20))\n",
        "model5.add(layers.GRU(20,dropout=0.5))\n",
        "model5.add(layers.Dense(3,activation='softmax'))\n",
        "\n",
        "\n",
        "model5.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpoint5 = ModelCheckpoint(\"best_model5.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
        "history = model5.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test),callbacks=[checkpoint1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwOcmTk5W5Gl"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtRGmZTxVvb_"
      },
      "source": [
        "model6 = Sequential()\n",
        "model6.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
        "model6.add(layers.Bidirectional(layers.GRU(20,dropout=0.6)))\n",
        "model6.add(layers.Dense(3,activation='softmax'))\n",
        "model6.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "#model2 = keras.models.load_model('./best_model2.hdf5')\n",
        "checkpoint6 = ModelCheckpoint(\"best_model6.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
        "history = model6.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test),callbacks=[checkpoint2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0zS6FeTWK9a"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTbLIsxzW_ZY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}